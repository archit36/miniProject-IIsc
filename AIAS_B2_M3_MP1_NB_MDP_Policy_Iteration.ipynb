{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Machine Learning and AI for Autonomous Systems\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project : Policy Iteration in Frozen lake enviroment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nK0fzdQzk0g"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the mini project, you will be able to\n",
        "\n",
        "* understand the Gym enviroment\n",
        "* use of policy iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hungry-accident"
      },
      "source": [
        "**Packages used:**  \n",
        "* `GYM` for data frames and easy to read csv files  \n",
        "* `Numpy` for array and matrix mathematics functions  \n",
        "* `Matplotlib` and `Seaborn` for visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umwh23IEbGSA"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment\n",
        "\n",
        "In this example we are using Frozen Lake environment. The Frozen Lake environment is a simple but classic reinforcement learning environment from the OpenAI Gym library. It is a text-based environment in which the agent starts at a starting position on a frozen lake and must navigate to the goal position without falling into any holes. The lake can be slippery, so the agent may not always move in the intended direction."
      ],
      "metadata": {
        "id": "KvcvofZWpU6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1 Get the transition probabilities and reward function for Frozen Lake environment(3 marks)"
      ],
      "metadata": {
        "id": "ms0q0YderggF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For testing the output of policy iteration (you can write your own code).\n",
        "def testPolicy(policy, trials=100):\n",
        "    \"\"\"\n",
        "    Get the average rate of successful episodes over given number of trials\n",
        "    : param policy: function, a deterministic policy function\n",
        "    : param trials: int, number of trials\n",
        "    : return: float, average success rate\n",
        "    \"\"\"\n",
        "    env = gym.make('FrozenLake-v1')\n",
        "    env.reset()\n",
        "    success = 0\n",
        "\n",
        "    for _ in range(trials):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, _, done, _ = env.step(action)\n",
        "            if state == 15:\n",
        "                success += 1\n",
        "\n",
        "    avg_success_rate = success / trials\n",
        "    return avg_success_rate"
      ],
      "metadata": {
        "id": "CRv9i5pLrrpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "XKuN9YZ_rtaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Iteration"
      ],
      "metadata": {
        "id": "0kqweAFysEyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Policy:\n",
        " A policy is the thought process behind picking an action. In practice, it’s a probability distribution assigned to the set of actions. Highly rewarding actions will have a high probability and vice versa. If an action has a low probability, it doesn’t mean it won’t be picked at all. It’s just less likely to be picked.\n",
        "\n",
        " Policy iteration is a dynamic programming algorithm for finding the optimal policy in a Markov decision process (MDP). It works by iteratively evaluating and improving the policy until convergence.\n",
        "\n",
        "**Policy evaluation**\n",
        "\n",
        "Policy evaluation takes a policy as input and computes the expected value of each state under that policy. This is done by recursively calculating the expected value of each state, taking into account the transition probabilities and rewards of the MDP.\n",
        "\n",
        "**Policy improvement**\n",
        "\n",
        "Policy improvement takes the value function as input and computes a new policy that is guaranteed to be at least as good as the old policy. This is done by choosing the action in each state that leads to the highest expected value.\n",
        "\n",
        "**Policy iteration algorithm**\n",
        "\n",
        "The policy iteration algorithm works as follows:\n",
        "\n",
        "1. **Initialize the value function.** This can be done arbitrarily.\n",
        "2. **Evaluate the policy.** Use the policy evaluation algorithm to compute the expected value of each state under the current policy.\n",
        "3. **Improve the policy.** Use the policy improvement algorithm to compute a new policy that is guaranteed to be at least as good as the old policy.\n",
        "4. **Repeat steps 2 and 3 until the policy converges.**\n",
        "\n",
        "**Convergence**\n",
        "\n",
        "The policy iteration algorithm is guaranteed to converge to the optimal policy in a finite number of iterations. This is because a finite MDP has only a finite number of policies, and each policy iteration either improves the policy or leaves it unchanged.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we have the following MDP:\n",
        "\n",
        "```\n",
        "State | Action | Transition probability | Reward\n",
        "-------|--------|-------------------|-------\n",
        "S1   | A     | 0.5, 0.5         | 1\n",
        "S1   | B     | 0.5, 0.5         | 0\n",
        "S2   | A     | 1             | 10\n",
        "S2   | B     | 1             | 0\n",
        "```\n",
        "\n",
        "The optimal policy is to always go to state S2, regardless of the current state. This is because state S2 has a higher expected reward than state S1.\n",
        "\n",
        "We can use policy iteration to find the optimal policy as follows:\n",
        "\n",
        "1. **Initialize the value function.** We can initialize the value function to all zeros.\n",
        "2. **Evaluate the policy.** We use the policy evaluation algorithm to compute the expected value of each state under the current policy.\n",
        "\n",
        "```\n",
        "State | Value\n",
        "-------|-------\n",
        "S1   | 0.5\n",
        "S2   | 10\n",
        "```\n",
        "\n",
        "3. **Improve the policy.** We use the policy improvement algorithm to compute a new policy that is guaranteed to be at least as good as the old policy.\n",
        "\n",
        "```\n",
        "State | Action\n",
        "-------|--------\n",
        "S1   | A\n",
        "S2   | A\n",
        "```\n",
        "\n",
        "4. **Repeat steps 2 and 3 until the policy converges.**\n",
        "\n",
        "We can see that the policy has converged, since the new policy is the same as the old policy. Therefore, the optimal policy is to always go to state S2, regardless of the current state.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Policy iteration is a powerful algorithm for finding the optimal policy in an MDP. It is guaranteed to converge to the optimal policy in a finite number of iterations. Policy iteration is often used in reinforcement learning to train agents to learn how to behave optimally in complex environments."
      ],
      "metadata": {
        "id": "EnOJEeTYr-te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2 Write the code for policy evaluation(2 marks)"
      ],
      "metadata": {
        "id": "399eOKaisODf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Z5ds7oq5sCNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3 Write the code for policy improvement(2 marks)"
      ],
      "metadata": {
        "id": "3TqxkBtvs4Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "8BPmiCCztFLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4 Write the code for policy iteration(2marks)"
      ],
      "metadata": {
        "id": "s-slbejy0p57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "aAeqHYF502BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5 Apply policy iteration in Frozen Lake environment(1 marks)"
      ],
      "metadata": {
        "id": "0NNFeV8r1Iku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "w_YUFd4K7HcC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}